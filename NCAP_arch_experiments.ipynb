{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "D4zDbigvE_rN"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/Macrocircuits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/projects/project-notebooks/Macrocircuits.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "c8RURvTPE_rN"
      },
      "source": [
        "# Macrocircuits\n",
        "\n",
        "***Macrocircuits: Leveraging neural architectural priors and modularity in embodied agents***\n",
        "\n",
        "**By Neuromatch Academy**\n",
        "\n",
        "**Content creators:** Divyansha Lachi, Kseniia Shilova  \n",
        "\n",
        "**Content reviewers:** Eva Dyer, Hannah Choi  \n",
        "\n",
        "__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "_YXklU7jE_rO"
      },
      "source": [
        "## Background\n",
        "This project explores how we can build a biologically inspired artificial neural network (ANN) architecture, derived from the C. Elegans motor circuit, for the control of a simulated Swimmer agent. Traditional motor control ANNs often rely on generic, fully connected multilayer perceptrons (MLPs), which demand extensive training data, offer limited transferability, and possess complex internal dynamics that challenge interpretability. The project aims to understand how the biologically motivated ANN, which is shaped by evolution to be highly structured and sparse, could help to solve these problems and provide advantages in the domain of motor control. We will train MLPs using algorithms such as PPO, DDPG, and ES, and compare their performance in terms of rewards and sample efficiency with our bio-inspired ANN. The project also includes visualizing the C. Elegans connectome and building the network using this circuitry. We will conduct various ablation analyses by removing sign and weight-sharing constraints, and altering environmental parameters like the swimmer’s length or viscosity. These investigations aim to understand how architecture and modularity impact performance and learning across different environments. Finally, the project aims at building an agent that is robust to environmental variations, navigating towards specific targets, and enhancing our understanding of bio-inspired motor control.  \n",
        "\n",
        "\n",
        "**Relevant references:**  \n",
        "\n",
        "- [Neural circuit architectural priors for embodied control](https://arxiv.org/abs/2201.05242)  \n",
        "- [Hierarchical motor control in mammals and machines](https://www.nature.com/articles/s41467-019-13239-6)  \n",
        "- [Continuous control with deep reinforcement learning](https://arxiv.org/pdf/1509.02971.pdf)  \n",
        "\n",
        "*This notebook uses code from the following GitHub repository:* [ncap](https://github.com/nikhilxb/ncap) by Nikhil X. Bhattasali and Anthony M. Zador and Tatiana A. Engel.\n",
        "\n",
        "**Infrastructure note:** This notebook contains GPU install guide as well as CPU ones for different OS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "HUhwIPLmE_rO"
      },
      "outputs": [],
      "source": [
        "# @title Install and import feedback gadget\n",
        "\n",
        "!pip install vibecheck datatops --quiet\n",
        "\n",
        "from vibecheck import DatatopsContentReviewContainer\n",
        "def content_review(notebook_section: str):\n",
        "    return DatatopsContentReviewContainer(\n",
        "        \"\",  # No text prompt\n",
        "        notebook_section,\n",
        "        {\n",
        "            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
        "            \"name\": \"neuromatch_neuroai\",\n",
        "            \"user_key\": \"wb2cxze8\",\n",
        "        },\n",
        "    ).render()\n",
        "\n",
        "feedback_prefix = \"Project_Macrocircuits\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "fpizcOxUE_rO"
      },
      "outputs": [],
      "source": [
        "# @title Project Background\n",
        "\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import YouTubeVideo\n",
        "from IPython.display import IFrame\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "class PlayVideo(IFrame):\n",
        "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
        "    self.id = id\n",
        "    if source == 'Bilibili':\n",
        "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
        "    elif source == 'Osf':\n",
        "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
        "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "\n",
        "def display_videos(video_ids, W=400, H=300, fs=1):\n",
        "  tab_contents = []\n",
        "  for i, video_id in enumerate(video_ids):\n",
        "    out = widgets.Output()\n",
        "    with out:\n",
        "      if video_ids[i][0] == 'Youtube':\n",
        "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
        "                             height=H, fs=fs, rel=0)\n",
        "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
        "      else:\n",
        "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
        "                          height=H, fs=fs, autoplay=False)\n",
        "        if video_ids[i][0] == 'Bilibili':\n",
        "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
        "        elif video_ids[i][0] == 'Osf':\n",
        "          print(f'Video available at https://osf.io/{video.id}')\n",
        "      display(video)\n",
        "    tab_contents.append(out)\n",
        "  return tab_contents\n",
        "\n",
        "\n",
        "video_ids = [('Youtube', 'CwOUn0Bt4JU'), ('Bilibili', 'BV1Zx4y1t7BU')]\n",
        "tab_contents = display_videos(video_ids, W=854, H=480)\n",
        "tabs = widgets.Tab()\n",
        "tabs.children = tab_contents\n",
        "for i in range(len(tab_contents)):\n",
        "  tabs.set_title(i, video_ids[i][0])\n",
        "display(tabs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "KW0C0uvYE_rP"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_project_background\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "sbFofLz7E_rP"
      },
      "outputs": [],
      "source": [
        "# @title Project slides\n",
        "\n",
        "from IPython.display import IFrame\n",
        "print(\"If you want to download the slides: https://osf.io/download/cz93b/\")\n",
        "IFrame(src=\"https://mfr.ca-1.osf.io/render?url=https://osf.io/download/cz93b/?direct%26mode=render\", width=854, height=480)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "tIm5AO0ZE_rP"
      },
      "outputs": [],
      "source": [
        "#@title Project Template\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "url = \"https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/NCAPProjectTemplate.png?raw=true\"\n",
        "\n",
        "display(Image(url=url))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "3Jcnlc3WE_rQ"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_project_template\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "XzBn5hHGE_rQ"
      },
      "source": [
        "**Tutorial links**\n",
        "\n",
        "This particular project connects a couple of distinct ideas explored throughout the course. Firstly, the innate ability to learn a certain set of actions quickly is the main topic of [Tutorial 4](https://neuroai.neuromatch.io/tutorials/W2D4_Macrolearning/student/W2D4_Tutorial4.html) for **W2D4** on biological meta-learning. The focus comes with the observation that the brain is not of a generic architecture but is a highly structured and optimized hierarchy of modules, the importance of which is highlighted in [Tutorial 3](https://neuroai.neuromatch.io/tutorials/W2D1_Macrocircuits/student/W2D1_Tutorial3.html) for **W2D1**, forming inductive bias for efficient motor control. The default model for the agent used here is already known Actor-Critic; you had the opportunity to observe in already mentioned tutorials as well as in [Tutorial 3](https://neuroai.neuromatch.io/tutorials/W1D2_ComparingTasks/student/W1D2_Tutorial3.html) for **W1D2**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "-VcrAUlQE_rQ"
      },
      "source": [
        "---\n",
        "## Scetion 0: Initial setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "gJEKqgSjE_rQ"
      },
      "source": [
        "**IF USING COLAB (recommended):**\n",
        "\n",
        "Uncomment the cell below and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "JlTqjmOOE_rQ"
      },
      "outputs": [],
      "source": [
        "#@title Installing Dependencies (Colab GPU case, uncomment if you want to use this one)\n",
        "\n",
        "import distutils.util\n",
        "import os\n",
        "import subprocess\n",
        "if subprocess.run('nvidia-smi').returncode:\n",
        "  raise RuntimeError(\n",
        "      'Cannot communicate with GPU. '\n",
        "      'Make sure you are using a GPU Colab runtime. '\n",
        "      'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
        "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
        "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
        "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
        "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "    f.write(\"\"\"{\n",
        "    \"file_format_version\" : \"1.0.0\",\n",
        "    \"ICD\" : {\n",
        "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "print('Installing dm_control...')\n",
        "!pip install -q dm_control>=1.0.16\n",
        "\n",
        "# Configure dm_control to use the EGL rendering backend (requires GPU)\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "!echo Installed dm_control $(pip show dm_control | grep -Po \"(?<=Version: ).+\")\n",
        "!pip install -q dm-acme[envs]\n",
        "!mkdir output_videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "9jYbYlIaE_rQ"
      },
      "source": [
        "**IF USING KAGGLE (recommended):**\n",
        "\n",
        "Uncomment the cell below and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "ktIGULJdE_rQ"
      },
      "outputs": [],
      "source": [
        "#@title Installing Dependencies (Kaggle GPU case, uncomment if you want to use this one)\n",
        "\n",
        "# import subprocess\n",
        "\n",
        "# subprocess.run([\"sudo\", \"apt-get\", \"install\", \"-y\", \"libgl1-mesa-glx\", \"libosmesa6\"])\n",
        "# subprocess.run([\"pip\", \"install\", \"-q\", \"imageio[ffmpeg]\"])\n",
        "\n",
        "# print('Installing dm_control...')\n",
        "# !pip install -q dm_control>=1.0.16\n",
        "\n",
        "# %env MUJOCO_GL=osmesa\n",
        "\n",
        "# !echo Installed dm_control $(pip show dm_control | grep -Po \"(?<=Version: ).+\")\n",
        "# !pip install -q dm-acme[envs]\n",
        "# !mkdir output_videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "rkASUXXVE_rQ"
      },
      "source": [
        " **IF RUNNING LOCALLY**\n",
        "\n",
        "Uncomment the relevant lines of code depending on your OS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "FbD-IE_mE_rQ"
      },
      "outputs": [],
      "source": [
        "#@title Installing Dependencies (CPU case, comment if you want to use GPU one)\n",
        "\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "############### For Linux #####################\n",
        "# subprocess.run([\"sudo\", \"apt-get\", \"install\", \"-y\", \"libglew-dev\"])\n",
        "# subprocess.run([\"sudo\", \"apt-get\", \"install\", \"-y\", \"libglfw3\"])\n",
        "# subprocess.run([\"sudo\", \"apt\", \"install\", \"ffmpeg\"])\n",
        "###############################################\n",
        "\n",
        "############### For MacOS #####################\n",
        "# subprocess.run([\"brew\", \"install\", \"glew\"])\n",
        "# subprocess.run([\"brew\", \"install\", \"glfw\"])\n",
        "###############################################\n",
        "\n",
        "subprocess.run([\"pip\", \"install\", \"-q\", \"ffmpeg\"])\n",
        "subprocess.run([\"pip\", \"install\", \"-q\", \"dm-acme[envs]\"])\n",
        "subprocess.run([\"pip\", \"install\", \"-q\", \"dm_control>=1.0.16\"])\n",
        "\n",
        "!mkdir output_videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "jmaGSVZRE_rR"
      },
      "source": [
        "**Imports and Utility Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "cobKebQ7E_rR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Importing Libraries\n",
        "import numpy as np\n",
        "import collections\n",
        "import argparse\n",
        "import os\n",
        "import yaml\n",
        "import typing as T\n",
        "import imageio\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from IPython.display import HTML\n",
        "import csv\n",
        "import dm_control as dm\n",
        "import dm_control.suite.swimmer as swimmer\n",
        "from dm_control.rl import control\n",
        "from dm_control.utils import rewards\n",
        "from dm_control import suite\n",
        "from dm_control.suite.wrappers import pixels\n",
        "\n",
        "from acme import wrappers\n",
        "\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "E7ZF-3dDE_rR"
      },
      "outputs": [],
      "source": [
        "#@title Utility code for displaying videos\n",
        "def write_video(\n",
        "  filepath: os.PathLike,\n",
        "  frames: T.Iterable[np.ndarray],\n",
        "  fps: int = 60,\n",
        "  macro_block_size: T.Optional[int] = None,\n",
        "  quality: int = 10,\n",
        "  verbose: bool = False,\n",
        "  **kwargs,\n",
        "):\n",
        "  \"\"\"\n",
        "  Saves a sequence of frames as a video file.\n",
        "\n",
        "  Parameters:\n",
        "  - filepath (os.PathLike): Path to save the video file.\n",
        "  - frames (Iterable[np.ndarray]): An iterable of frames, where each frame is a numpy array.\n",
        "  - fps (int, optional): Frames per second, defaults to 60.\n",
        "  - macro_block_size (Optional[int], optional): Macro block size for video encoding, can affect compression efficiency.\n",
        "  - quality (int, optional): Quality of the output video, higher values indicate better quality.\n",
        "  - verbose (bool, optional): If True, prints the file path where the video is saved.\n",
        "  - **kwargs: Additional keyword arguments passed to the imageio.get_writer function.\n",
        "\n",
        "  Returns:\n",
        "  None. The video is written to the specified filepath.\n",
        "  \"\"\"\n",
        "\n",
        "  with imageio.get_writer(filepath,\n",
        "                        fps=fps,\n",
        "                        macro_block_size=macro_block_size,\n",
        "                        quality=quality,\n",
        "                        **kwargs) as video:\n",
        "    if verbose: print('Saving video to:', filepath)\n",
        "    for frame in frames:\n",
        "      video.append_data(frame)\n",
        "\n",
        "\n",
        "def display_video(\n",
        "  frames: T.Iterable[np.ndarray],\n",
        "  filename='output_videos/temp.mp4',\n",
        "  fps=60,\n",
        "  **kwargs,\n",
        "):\n",
        "  \"\"\"\n",
        "  Displays a video within a Jupyter Notebook from an iterable of frames.\n",
        "\n",
        "  Parameters:\n",
        "  - frames (Iterable[np.ndarray]): An iterable of frames, where each frame is a numpy array.\n",
        "  - filename (str, optional): Temporary filename to save the video before display, defaults to 'output_videos/temp.mp4'.\n",
        "  - fps (int, optional): Frames per second for the video display, defaults to 60.\n",
        "  - **kwargs: Additional keyword arguments passed to the write_video function.\n",
        "\n",
        "  Returns:\n",
        "  HTML object: An HTML video element that can be displayed in a Jupyter Notebook.\n",
        "  \"\"\"\n",
        "\n",
        "  # Write video to a temporary file.\n",
        "  filepath = os.path.abspath(filename)\n",
        "  write_video(filepath, frames, fps=fps, verbose=False, **kwargs)\n",
        "\n",
        "  height, width, _ = frames[0].shape\n",
        "  dpi = 70\n",
        "  orig_backend = matplotlib.get_backend()\n",
        "  matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
        "  matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
        "  ax.set_axis_off()\n",
        "  ax.set_aspect('equal')\n",
        "  ax.set_position([0, 0, 1, 1])\n",
        "  im = ax.imshow(frames[0])\n",
        "  def update(frame):\n",
        "    im.set_data(frame)\n",
        "    return [im]\n",
        "  interval = 1000/fps\n",
        "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                  interval=interval, blit=True, repeat=False)\n",
        "  return HTML(anim.to_html5_video())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "fOj3yR7iE_rR"
      },
      "source": [
        "\n",
        "In this notebook we will explore the major components essential for this project.\n",
        "\n",
        "\n",
        "*   **Understanding the DeepMind Control Suite Swimmer Agent:** We will begin by exploring the swimmer agent provided by the DeepMind Control Suite. This section includes a detailed exploration of the agent's API, task customization capabilities, and how to adapt the environment to fit our experimental needs.\n",
        "*   **Training Models Using Various Reinforcement Learning Algorithms:** Next, we move on to learn how can we train models for the agents we created. We will be using Tonic_RL library to train our model. We will first train a standard MLP model using the Proximal Policy Optimization (PPO) algorithm.\n",
        "\n",
        "* **Training the NCAP model:** Finally we will define the NCAP model from [Neural Circuit Architectural Priors for Embodied Control](https://arxiv.org/abs/2201.05242) paper. We will train it using PPO and compare it against the MLP model we trained before.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "Tnkc-0kME_rR"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_initial_setup\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "rVtube7GE_rR"
      },
      "source": [
        "---\n",
        "## Section 1: Exploring the DeepMind Swimmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "bkNAjp9PE_rR"
      },
      "source": [
        "### 1.1 Create a basic swim task for the swimmer environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "qWv2m2nDE_rR"
      },
      "source": [
        "First, we'll initialize a basic swimmer agent consisting of 6 links. Each agent requires a defined task and its corresponding reward function. In this instance, we've designed a swim forward task that involves the agent swimming forward in any direction.\n",
        "\n",
        "The environment is flexible, allowing for modifications to introduce additional tasks such as \"swim only in the x-direction\" or \"move towards a ball.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "ZizmGqLvE_rR"
      },
      "outputs": [],
      "source": [
        "_SWIM_SPEED = 0.1\n",
        "\n",
        "@swimmer.SUITE.add()\n",
        "def swim(\n",
        "  n_links=6,\n",
        "  desired_speed=_SWIM_SPEED,\n",
        "  time_limit=swimmer._DEFAULT_TIME_LIMIT,\n",
        "  random=None,\n",
        "  environment_kwargs={},\n",
        "):\n",
        "  \"\"\"Returns the Swim task for a n-link swimmer.\"\"\"\n",
        "  model_string, assets = swimmer.get_model_and_assets(n_links)\n",
        "  physics = swimmer.Physics.from_xml_string(model_string, assets=assets)\n",
        "  task = Swim(desired_speed=desired_speed, random=random)\n",
        "  return control.Environment(\n",
        "    physics,\n",
        "    task,\n",
        "    time_limit=time_limit,\n",
        "    control_timestep=swimmer._CONTROL_TIMESTEP,\n",
        "    **environment_kwargs,\n",
        "  )\n",
        "\n",
        "\n",
        "class Swim(swimmer.Swimmer):\n",
        "  \"\"\"Task to swim forwards at the desired speed.\"\"\"\n",
        "  def __init__(self, desired_speed=_SWIM_SPEED, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self._desired_speed = desired_speed\n",
        "\n",
        "  def initialize_episode(self, physics):\n",
        "    super().initialize_episode(physics)\n",
        "    # Hide target by setting alpha to 0.\n",
        "    physics.named.model.mat_rgba['target', 'a'] = 0\n",
        "    physics.named.model.mat_rgba['target_default', 'a'] = 0\n",
        "    physics.named.model.mat_rgba['target_highlight', 'a'] = 0\n",
        "\n",
        "  def get_observation(self, physics):\n",
        "    \"\"\"Returns an observation of joint angles and body velocities.\"\"\"\n",
        "    obs = collections.OrderedDict()\n",
        "    obs['joints'] = physics.joints()\n",
        "    obs['body_velocities'] = physics.body_velocities()\n",
        "    return obs\n",
        "\n",
        "  def get_reward(self, physics):\n",
        "    \"\"\"Returns a smooth reward that is 0 when stopped or moving backwards, and rises linearly to 1\n",
        "    when moving forwards at the desired speed.\"\"\"\n",
        "    forward_velocity = -physics.named.data.sensordata['head_vel'][1]\n",
        "    return rewards.tolerance(\n",
        "      forward_velocity,\n",
        "      bounds=(self._desired_speed, float('inf')),\n",
        "      margin=self._desired_speed,\n",
        "      value_at_margin=0.,\n",
        "      sigmoid='linear',\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "bYEVTI4YE_rR"
      },
      "source": [
        "### 1.2 Vizualizing an agent that takes random actions in the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "iLOk_R-CE_rR"
      },
      "source": [
        "Let's visualize the environment by executing a sequence of random actions on a swimmer agent. This involves applying random actions over a series of steps and compiling the rendered frames into a video to visualize the agent's behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "9aQzBMTME_rR"
      },
      "outputs": [],
      "source": [
        "\"\"\" Renders the current environment state to an image \"\"\"\n",
        "def render(env):\n",
        "    return env.physics.render(camera_id=0, width=640, height=480)\n",
        "\n",
        "def test_dm_control(env_name, task_name, steps=60, random_seed=1):\n",
        "    env = suite.load(env_name, task_name, task_kwargs={'random': random_seed})\n",
        "    env = wrappers.CanonicalSpecWrapper(env, clip=True)\n",
        "    env = wrappers.SinglePrecisionWrapper(env)\n",
        "\n",
        "    spec = env.action_spec()\n",
        "    timestep = env.reset()\n",
        "    frames = [render(env)]\n",
        "\n",
        "    for _ in range(steps):\n",
        "        action = np.random.uniform(low=spec.minimum, high=spec.maximum, size=spec.shape)\n",
        "        timestep = env.step(action)\n",
        "        frames.append(render(env))\n",
        "    return frames, env\n",
        "\n",
        "frames, env = test_dm_control('swimmer', 'swimmer6', steps=60, random_seed=1)\n",
        "display_video(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "wCaEZAhrE_rR"
      },
      "source": [
        "### 1.3 Swimmer Agent API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "jtPQSKXGE_rR"
      },
      "source": [
        "The observation space consists of 25 total dimensions, combining joint positions and body velocities, while the action space involves 5 dimensions representing normalized joint forces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "BEsUSIzuE_rS"
      },
      "source": [
        "\n",
        "**Observation Space**: 4k - 1 total (k = 6 $\\rightarrow$ 23)\n",
        "\n",
        "- k - 1: joint positions $q_i \\in [-\\pi, \\pi]$ (`joints`)\n",
        "- 3k: link linear velocities $vx_i, vy_i \\in \\mathbb{R}$ and rotational velocity $wz_i \\in \\mathbb{R}$ (`body_velocities`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "rqtF8hkuE_rS"
      },
      "outputs": [],
      "source": [
        "env.observation_spec()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "EbbR9oJxE_rS"
      },
      "source": [
        "**Action Space**: k - 1 total (k = 6 $\\rightarrow$ 5)\n",
        "\n",
        "- k - 1: joint normalized force $\\ddot{q}_i \\in [-1, 1]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "Ym_KKcAeE_rS"
      },
      "outputs": [],
      "source": [
        "env.action_spec()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "OZJnX2uRE_rS"
      },
      "source": [
        "### 1.4 Example of simple modification to the agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "cE-MEh2wE_rS"
      },
      "source": [
        "\n",
        "Let's make a new swimmer agent with 12 links instead of 6, introducing complexity. Additionally, we have the flexibility to adjust various other parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "ShOeNb3OE_rS"
      },
      "outputs": [],
      "source": [
        "@swimmer.SUITE.add()\n",
        "def swim_12_links(\n",
        "  n_links=12,\n",
        "  desired_speed=_SWIM_SPEED,\n",
        "  time_limit=swimmer._DEFAULT_TIME_LIMIT,\n",
        "  random=None,\n",
        "  environment_kwargs={},\n",
        "):\n",
        "  \"\"\"Returns the Swim task for a n-link swimmer.\"\"\"\n",
        "  model_string, assets = swimmer.get_model_and_assets(n_links)\n",
        "  physics = swimmer.Physics.from_xml_string(model_string, assets=assets)\n",
        "  task = Swim(desired_speed=desired_speed, random=random)\n",
        "  return control.Environment(\n",
        "    physics,\n",
        "    task,\n",
        "    time_limit=time_limit,\n",
        "    control_timestep=swimmer._CONTROL_TIMESTEP,\n",
        "    **environment_kwargs,\n",
        "  )\n",
        "\n",
        "swim_12_frames, swim_12_env = test_dm_control('swimmer', 'swim_12_links', steps=60, random_seed=1)\n",
        "display_video(swim_12_frames)\n",
        "# env = suite.load('swimmer', 'swim_12_links', task_kwargs={'random': 1})\n",
        "# test_dm_control(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "R8eUOFUkE_rS"
      },
      "source": [
        "We can visualize this longer agent using our previously defined test_dm_control function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "o8l2hd80E_rS"
      },
      "source": [
        "❓"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ljp-QyM3E_rS"
      },
      "source": [
        "Using the API provided by Deepmind we can create any kind of changes to the agent and the environment.\n",
        "\n",
        "Try to make the following changes to make yourself more familiar with the swimmer.\n",
        "\n",
        "*   ***Adding a target (like a ball) to this environment at some x distance away from the agent.***\n",
        "*   ***Increasing the viscosity of the environment.***\n",
        "\n",
        "\n",
        "\n",
        "Have a look at the following links to see what kind of assets you will need to modify to make these changes.\n",
        "\n",
        "\n",
        "*   [swimmer.py](https://github.com/google-deepmind/dm_control/blob/main/dm_control/suite/swimmer.py)\n",
        "*   [swimmer.xml](https://github.com/google-deepmind/dm_control/blob/main/dm_control/suite/swimmer.xml)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "8Qnkm37aE_rS"
      },
      "outputs": [],
      "source": [
        "# add your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "jGLTaOmcE_rS"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_exploring_deepmind_swimmer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "S7xLNktWE_rS"
      },
      "source": [
        "---\n",
        "## Section 2: Training models on the swim task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "0wXZJGdHE_rS"
      },
      "source": [
        "\n",
        "To train the agents we defined in the previous section, we will utilize standard reinforcement learning (RL) algorithms. For the purposes of this tutorial, we will employ the [tonic_rl](https://github.com/fabiopardo/tonic) library, which provides a robust framework for training RL agents. Throughout most of this project, you will primarily be modifying the environment or the model architecture. Therefore, I suggest treating these algorithms as a \"black box\" for now. Simply put, you input an untrained model, and the algorithm processes and returns a well-trained model. This approach allows us to focus on the impact of different architectures and environmental settings without delving deeply into the algorithmic complexities at this stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "__tkaYeME_rS"
      },
      "outputs": [],
      "source": [
        "#@title Download and install tonic library for training agents\n",
        "\n",
        "import contextlib\n",
        "import io\n",
        "\n",
        "\n",
        "!git clone https://github.com/neuromatch/tonic\n",
        "%cd tonic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "-y9a3UadE_rT"
      },
      "source": [
        "### Section 2.1 Defining the train function\n",
        "\n",
        "First we defined a general training function to train any agent on any given environment with a variety of available algorithms. Given below are some of the parameter definitions of the function. You'll likely want to adjust these parameters to customize the training process for an agent in a specific environment using your chosen algorithm from the tonic library:\n",
        "\n",
        "* **Header**: Python code required to run before training begins, primarily for importing essential libraries or modules.\n",
        "\n",
        "* **Agent**: The agent that will undergo training; refer to section 3.2 and 4.2 for definitions of MLP and NCAP respectively.\n",
        "\n",
        "* **Environment**: The training environment for the agent. Ensure it is registered with the DeepMind Control Suite as detailed in section 2.\n",
        "\n",
        "* **Name**: The experiment's name, which will be utilized for log and model saving purposes.\n",
        "\n",
        "* **Trainer**: The trainer instance selected for use. It allows the configuration of the training steps, model saving frequency, and other training-related parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "YYCCOWfNE_rT"
      },
      "outputs": [],
      "source": [
        "import tonic\n",
        "import tonic.torch\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def train(\n",
        "  header,\n",
        "  agent,\n",
        "  environment,\n",
        "  name = 'test',\n",
        "  trainer = 'tonic.Trainer()',\n",
        "  before_training = None,\n",
        "  after_training = None,\n",
        "  parallel = 1,\n",
        "  sequential = 1,\n",
        "  seed = 0\n",
        "):\n",
        "  \"\"\"\n",
        "  Some additional parameters:\n",
        "\n",
        "  - before_training: Python code to execute immediately before the training loop commences, suitable for setup actions needed after initialization but prior to training.\n",
        "  - after_training: Python code to run once the training loop concludes, ideal for teardown or analytical purposes.\n",
        "  - parallel: The count of environments to execute in parallel. Limited to 1 in a Colab notebook, but if additional resources are available, this number can be increased to expedite training.\n",
        "  - sequential: The number of sequential steps the environment runs before sending observations back to the agent. This setting is useful for temporal batching. It can be disregarded for this tutorial's purposes.\n",
        "  - seed: The experiment's random seed, guaranteeing the reproducibility of the training process.\n",
        "\n",
        "  \"\"\"\n",
        "  # Capture the arguments to save them, e.g. to play with the trained agent.\n",
        "  args = dict(locals())\n",
        "\n",
        "  # Run the header first, e.g. to load an ML framework.\n",
        "  if header:\n",
        "    exec(header)\n",
        "\n",
        "  # Build the train and test environments.\n",
        "  _environment = environment\n",
        "  environment = tonic.environments.distribute(lambda: eval(_environment), parallel, sequential)\n",
        "  test_environment = tonic.environments.distribute(lambda: eval(_environment))\n",
        "\n",
        "\n",
        "  # Build the agent.\n",
        "  agent = eval(agent)\n",
        "  agent.initialize(\n",
        "    observation_space=test_environment.observation_space,\n",
        "    action_space=test_environment.action_space, seed=seed)\n",
        "\n",
        "  # Choose a name for the experiment.\n",
        "  if hasattr(test_environment, 'name'):\n",
        "    environment_name = test_environment.name\n",
        "  else:\n",
        "    environment_name = test_environment.__class__.__name__\n",
        "  if not name:\n",
        "    if hasattr(agent, 'name'):\n",
        "      name = agent.name\n",
        "    else:\n",
        "      name = agent.__class__.__name__\n",
        "    if parallel != 1 or sequential != 1:\n",
        "      name += f'-{parallel}x{sequential}'\n",
        "\n",
        "  # Initialize the logger to save data to the path environment/name/seed.\n",
        "  # add you drive path '/content/drive/My Drive/your_directory_name'\n",
        "  path = os.path.join('/content/drive/My Drive/', 'data', 'experiments', 'tonic', environment_name, name)\n",
        "  print (f\"path for saving logs/model --- {path}\")\n",
        "  tonic.logger.initialize(path, script_path=None, config=args)\n",
        "\n",
        "  # Build the trainer.\n",
        "  trainer = eval(trainer)\n",
        "  trainer.initialize(\n",
        "    agent=agent,\n",
        "    environment=environment,\n",
        "    test_environment=test_environment,\n",
        "  )\n",
        "  # Run some code before training.\n",
        "  if before_training:\n",
        "    exec(before_training)\n",
        "\n",
        "  # Train.\n",
        "  trainer.run()\n",
        "\n",
        "  # Run some code after training.\n",
        "  if after_training:\n",
        "    exec(after_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "dB2lL7YfE_rT"
      },
      "source": [
        "### Section 2.2 Training MLP model on swim task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "vRcxfgKuE_rT"
      },
      "source": [
        "Now we are going to define a function for creating an actor-critic model suitable for Proximal Policy Optimization (PPO) using a Multi-Layer Perceptron (MLP) architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "Sk4Oz8G4E_rT"
      },
      "outputs": [],
      "source": [
        "from tonic.torch import models, normalizers\n",
        "import torch\n",
        "\n",
        "def ppo_mlp_model(\n",
        "  actor_sizes=(64, 64),\n",
        "  actor_activation=torch.nn.Tanh,\n",
        "  critic_sizes=(64, 64),\n",
        "  critic_activation=torch.nn.Tanh,\n",
        "):\n",
        "\n",
        "  \"\"\"\n",
        "  Constructs an ActorCritic model with specified architectures for the actor and critic networks.\n",
        "\n",
        "  Parameters:\n",
        "  - actor_sizes (tuple): Sizes of the layers in the actor MLP.\n",
        "  - actor_activation (torch activation): Activation function used in the actor MLP.\n",
        "  - critic_sizes (tuple): Sizes of the layers in the critic MLP.\n",
        "  - critic_activation (torch activation): Activation function used in the critic MLP.\n",
        "\n",
        "  Returns:\n",
        "  - models.ActorCritic: An ActorCritic model comprising an actor and a critic with MLP torsos,\n",
        "    equipped with a Gaussian policy head for the actor and a value head for the critic,\n",
        "    along with observation normalization.\n",
        "  \"\"\"\n",
        "\n",
        "  return models.ActorCritic(\n",
        "    actor=models.Actor(\n",
        "      encoder=models.ObservationEncoder(),\n",
        "      torso=models.MLP(actor_sizes, actor_activation),\n",
        "      head=models.DetachedScaleGaussianPolicyHead(),\n",
        "    ),\n",
        "    critic=models.Critic(\n",
        "      encoder=models.ObservationEncoder(),\n",
        "      torso=models.MLP(critic_sizes, critic_activation),\n",
        "      head=models.ValueHead(),\n",
        "    ),\n",
        "    observation_normalizer=normalizers.MeanStd(),\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Ixko82cuE_rT"
      },
      "source": [
        "Next we call the train function which initiates the training process for the provided agent using the Tonic library. It specifies the components necessary for training, including the model, environment, and training parameters:\n",
        "\n",
        "**Agent**: A Proximal Policy Optimization (PPO) agent with a custom Multi-Layer Perceptron (MLP) model architecture, configured with 256 units in each of two layers for both the actor and the critic.\n",
        "\n",
        "**Environment**: The training environment is set to \"swimmer-swim\" from the Control Suite, a benchmark suite for continuous control tasks.\n",
        "\n",
        "**Name**: The experiment is named 'mlp_256', which is useful for identifying logs and saved models associated with this training run.\n",
        "\n",
        "**Trainer**: Specifies the training configuration, including the total number of steps (5e5) and the frequency of saving the model (1e5 steps)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Wj8pTARlE_rT"
      },
      "source": [
        "\n",
        "*Note:* The model will checkpoint every 'save_steps' amount of training steps*  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "6ztBCyuBE_rT"
      },
      "source": [
        "⏳\n",
        "\n",
        "The model can take some time to train so feel free to skip the training for now. We have provided the pretrained model for you to play with. Move on to the next section to vizualize a agent with the pretrained model.\n",
        "\n",
        "Uncomment the cell below if you want to perform the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "LiUau422E_rT"
      },
      "outputs": [],
      "source": [
        "# train('import tonic.torch',\n",
        "#       'tonic.torch.agents.PPO(model=ppo_mlp_model(actor_sizes=(256, 256), critic_sizes=(256,256)))',\n",
        "#       'tonic.environments.ControlSuite(\"swimmer-swim\")',\n",
        "#       name = 'mlp_256',\n",
        "#       trainer = 'tonic.Trainer(steps=int(5e5),save_steps=int(1e5))')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "hfJyHQh4E_rT"
      },
      "source": [
        "❓"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "cMwoHEI_E_rT"
      },
      "source": [
        "**Try playing with the parameters of the trainer and the MLP model and see how it affects the performance.**\n",
        "\n",
        "\n",
        "*   ***How do the actor and the critic model size affect the performance.***\n",
        "*   ***Consider increasing the number of steps in trainer to train the model for longer.***\n",
        "*   ***Explore [Tonic library](https://github.com/fabiopardo/tonic) to see what algorithms we can use to train our agents. (D4PG is usually faster than PPO)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "s21E27eeE_rT"
      },
      "outputs": [],
      "source": [
        "# add your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "nQd_7m4KE_rT"
      },
      "source": [
        "### Section 2.3 Function to run any model on the environment and generate video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Rlpuf8rSE_rT"
      },
      "source": [
        "One of the most fun things about these environments is their visualization. We don't want to just look at the reward to know how good our model is we want to see how well the agent swims. This is particularly important to avoid \"reward hacking,\" where an agent learns to exploit the reward system in ways that are unintended and potentially detrimental to the desired outcomes. Moreover visualizing the agent also help us understand where the model is going wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Za69f4JnE_rT"
      },
      "source": [
        "Here we have defined a function that will generate the videos of the agent using the input model. The function requires path to the checkpoint folder and the environment you wanna run the trained model on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "g9gS-KJ-E_rT"
      },
      "outputs": [],
      "source": [
        "def play_model(path, checkpoint='last',environment='default',seed=None, header=None):\n",
        "\n",
        "  \"\"\"\n",
        "    Plays a model within an environment and renders the gameplay to a video.\n",
        "\n",
        "    Parameters:\n",
        "    - path (str): Path to the directory containing the model and checkpoints.\n",
        "    - checkpoint (str): Specifies which checkpoint to use ('last', 'first', or a specific ID). 'none' indicates no checkpoint.\n",
        "    - environment (str): The environment to use. 'default' uses the environment specified in the configuration file.\n",
        "    - seed (int): Optional seed for reproducibility.\n",
        "    - header (str): Optional Python code to execute before initializing the model, such as importing libraries.\n",
        "    \"\"\"\n",
        "\n",
        "  if checkpoint == 'none':\n",
        "    # Use no checkpoint, the agent is freshly created.\n",
        "    checkpoint_path = None\n",
        "    tonic.logger.log('Not loading any weights')\n",
        "  else:\n",
        "    checkpoint_path = os.path.join(path, 'checkpoints')\n",
        "    if not os.path.isdir(checkpoint_path):\n",
        "      tonic.logger.error(f'{checkpoint_path} is not a directory')\n",
        "      checkpoint_path = None\n",
        "\n",
        "    # List all the checkpoints.\n",
        "    checkpoint_ids = []\n",
        "    for file in os.listdir(checkpoint_path):\n",
        "      if file[:5] == 'step_':\n",
        "        checkpoint_id = file.split('.')[0]\n",
        "        checkpoint_ids.append(int(checkpoint_id[5:]))\n",
        "\n",
        "    if checkpoint_ids:\n",
        "      if checkpoint == 'last':\n",
        "        # Use the last checkpoint.\n",
        "        checkpoint_id = max(checkpoint_ids)\n",
        "        checkpoint_path = os.path.join(checkpoint_path, f'step_{checkpoint_id}')\n",
        "      elif checkpoint == 'first':\n",
        "        # Use the first checkpoint.\n",
        "        checkpoint_id = min(checkpoint_ids)\n",
        "        checkpoint_path = os.path.join(checkpoint_path, f'step_{checkpoint_id}')\n",
        "      else:\n",
        "        # Use the specified checkpoint.\n",
        "        checkpoint_id = int(checkpoint)\n",
        "        if checkpoint_id in checkpoint_ids:\n",
        "          checkpoint_path = os.path.join(checkpoint_path, f'step_{checkpoint_id}')\n",
        "        else:\n",
        "          tonic.logger.error(f'Checkpoint {checkpoint_id} not found in {checkpoint_path}')\n",
        "          checkpoint_path = None\n",
        "    else:\n",
        "      tonic.logger.error(f'No checkpoint found in {checkpoint_path}')\n",
        "      checkpoint_path = None\n",
        "\n",
        "  # Load the experiment configuration.\n",
        "  arguments_path = os.path.join(path, 'config.yaml')\n",
        "  with open(arguments_path, 'r') as config_file:\n",
        "    config = yaml.load(config_file, Loader=yaml.FullLoader)\n",
        "  config = argparse.Namespace(**config)\n",
        "\n",
        "  # Run the header first, e.g. to load an ML framework.\n",
        "  try:\n",
        "    if config.header:\n",
        "      exec(config.header)\n",
        "    if header:\n",
        "      exec(header)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  # Build the agent.\n",
        "  agent = eval(config.agent)\n",
        "\n",
        "  # Build the environment.\n",
        "  if environment == 'default':\n",
        "    environment  = tonic.environments.distribute(lambda: eval(config.environment))\n",
        "  else:\n",
        "    environment  = tonic.environments.distribute(lambda: eval(environment))\n",
        "  if seed is not None:\n",
        "    environment.seed(seed)\n",
        "\n",
        "  # Initialize the agent.\n",
        "  agent.initialize(\n",
        "    observation_space=environment.observation_space,\n",
        "    action_space=environment.action_space,\n",
        "    seed=seed,\n",
        "  )\n",
        "\n",
        "  # Load the weights of the agent form a checkpoint.\n",
        "  if checkpoint_path:\n",
        "    agent.load(checkpoint_path)\n",
        "\n",
        "  steps = 0\n",
        "  test_observations = environment.start()\n",
        "  frames = [environment.render('rgb_array',camera_id=0, width=640, height=480)[0]]\n",
        "  score, length = 0, 0\n",
        "\n",
        "  while True:\n",
        "      # Select an action.\n",
        "      actions = agent.test_step(test_observations, steps)\n",
        "      assert not np.isnan(actions.sum())\n",
        "\n",
        "      # Take a step in the environment.\n",
        "      test_observations, infos = environment.step(actions)\n",
        "      frames.append(environment.render('rgb_array',camera_id=0, width=640, height=480)[0])\n",
        "      agent.test_update(**infos, steps=steps)\n",
        "\n",
        "      score += infos['rewards'][0]\n",
        "      length += 1\n",
        "\n",
        "      if infos['resets'][0]:\n",
        "          break\n",
        "  video_path = os.path.join(path, 'video.mp4')\n",
        "  print('Reward for the run: ', score)\n",
        "  return display_video(frames,video_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "58EDo9SKE_rT"
      },
      "source": [
        "Let's visualize the agent with a pretrained MLP model. Once you have your pretrained model, you can replace the experiment path to visualize the agent with your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "gOLMv4OAE_rU"
      },
      "source": [
        "⏳"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "BJSJmRgxE_rU"
      },
      "outputs": [],
      "source": [
        "# play_model('data/local/experiments/tonic/swimmer-swim/mlp_256')\n",
        "#play_model('data/local/experiments/tonic/swimmer-swim/pretrained_mlp_ppo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "RVGA98ZpE_rU"
      },
      "source": [
        "❓"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "N0TcMTNTE_rU"
      },
      "source": [
        "Try testing the model on a modification of the enviroment it was trained on.\n",
        "\n",
        "\n",
        "*   ***Train on basic swim task and test on a environment with higher viscosity.***\n",
        "*   ***Can we train on the basic 6 link swimmer and test on a larger 12 link swimmer?***\n",
        "* ***Train the model for a bit on the modified environment and see how quickly the model can adapt to the new environment.***\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "W_g284RdE_rU"
      },
      "outputs": [],
      "source": [
        "# add your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "XmpGHBWKE_rU"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_training_models_on_swim_task\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "B1g-djr5E_rU"
      },
      "source": [
        "---\n",
        "## Section 3: NCAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "t4Gns2Q9E_rU"
      },
      "source": [
        "Now that we are familiar with how to train standard models on the swimmer agent let's take a look at NCAP a model that was inspired from the C. elegans motor circuit.\n",
        "Our hope with using such models is that they would already have really good priors which should lead to much better transfer, faster learning curves and possibly really good innate performance (zero shot performance).\n",
        "\n",
        "Check out [NCAP paper](https://arxiv.org/abs/2201.05242) to learn more about the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "U5LLR_1iE_rU"
      },
      "outputs": [],
      "source": [
        "#@title Paper Illustration\n",
        "\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "url = \"https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/NCAPPaper.png?raw=true\"\n",
        "\n",
        "display(Image(url=url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "26mIPgugE_rU"
      },
      "source": [
        "### 3.1 NCAP classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Ts2WEhMAE_rU"
      },
      "source": [
        "Now we are going to define ***SwimmerModule*** (NCAP model) and ***SwimmerActor*** (wrapper around NCAP model to make it compatible with tonic) classes.\n",
        "\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "9IMT6Q8BE_rU"
      },
      "source": [
        "#### Section 3.1.1 Defining the constraints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "a52IsBypE_rU"
      },
      "outputs": [],
      "source": [
        "# ==================================================================================================\n",
        "# Weight constraints.\n",
        "\n",
        "\n",
        "def excitatory(w, upper=None):\n",
        "    return w.clamp(min=0, max=upper)\n",
        "\n",
        "\n",
        "def inhibitory(w, lower=None):\n",
        "    return w.clamp(min=lower, max=0)\n",
        "\n",
        "\n",
        "def unsigned(w, lower=None, upper=None):\n",
        "    return w if lower is None and upper is None else w.clamp(min=lower, max=upper)\n",
        "\n",
        "\n",
        "# ==================================================================================================\n",
        "# Activation constraints.\n",
        "\n",
        "\n",
        "def graded(x):\n",
        "    return x.clamp(min=0, max=1)\n",
        "\n",
        "\n",
        "# ==================================================================================================\n",
        "# Weight initialization.\n",
        "\n",
        "\n",
        "def excitatory_uniform(shape=(1,), lower=0., upper=1.):\n",
        "    assert lower >= 0\n",
        "    # param = nn.init.xavier_uniform_(nn.Parameter(torch.empty(shape)))\n",
        "    # bounded_param = torch.clamp(param, min=lower, max=upper)  # Clamp values to specified range\n",
        "    # return torch.nn.Parameter(bounded_param)\n",
        "    #return nn.init.xavier_uniform_(nn.Parameter(torch.empty(shape)))\n",
        "    return nn.init.uniform_(nn.Parameter(torch.empty(shape)), a=lower, b=upper)\n",
        "\n",
        "\n",
        "def inhibitory_uniform(shape=(1,), lower=-1., upper=0.):\n",
        "    assert upper <= 0\n",
        "    # param = nn.init.xavier_uniform_(nn.Parameter(torch.empty(shape)))\n",
        "    # bounded_param = torch.clamp(param, min=lower, max=upper)\n",
        "    # return torch.nn.Parameter(bounded_param)\n",
        "    #return nn.init.xavier_uniform_(nn.Parameter(torch.empty(shape)))\n",
        "    return nn.init.uniform_(nn.Parameter(torch.empty(shape)), a=lower, b=upper)\n",
        "\n",
        "\n",
        "def unsigned_uniform(shape=(1,), lower=-1., upper=1.):\n",
        "    # param = nn.init.xavier_uniform_(nn.Parameter(torch.empty(shape)))\n",
        "    # bounded_param = torch.clamp(param, min=lower, max=upper)\n",
        "    # return torch.nn.Parameter(bounded_param)\n",
        "    #return nn.init.xavier_uniform_(nn.Parameter(torch.empty(shape)))\n",
        "    return nn.init.uniform_(nn.Parameter(torch.empty(shape)), a=lower, b=upper)\n",
        "\n",
        "\n",
        "def excitatory_constant(shape=(1,), value=1.):\n",
        "    return nn.Parameter(torch.full(shape, value))\n",
        "\n",
        "\n",
        "def inhibitory_constant(shape=(1,), value=-1.):\n",
        "    return nn.Parameter(torch.full(shape, value))\n",
        "\n",
        "\n",
        "def unsigned_constant(shape=(1,), lower=-1., upper=1., p=0.5):\n",
        "    with torch.no_grad():\n",
        "        weight = torch.empty(shape).uniform_(0, 1)\n",
        "        mask = weight < p\n",
        "        weight[mask] = upper\n",
        "        weight[~mask] = lower\n",
        "        return nn.Parameter(weight)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the distribution\n",
        "# taking 1000 samples\n",
        "shape = (1000, 1)\n",
        "\n",
        "# Generate weights for each distribution\n",
        "weights_excitatory = excitatory_uniform(shape)\n",
        "weights_inhibitory = inhibitory_uniform(shape)\n",
        "weights_unsigned = unsigned_uniform(shape)\n",
        "\n",
        "# Convert torch tensors to numpy arrays for plotting\n",
        "weights_excitatory_np = weights_excitatory.detach().numpy()\n",
        "weights_inhibitory_np = weights_inhibitory.detach().numpy()\n",
        "weights_unsigned_np = weights_unsigned.detach().numpy()\n",
        "\n",
        "# Plotting the histograms\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "# Excitatory uniform weights\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(weights_excitatory_np, bins=50, density=True, alpha=0.7, color='blue', edgecolor='black')\n",
        "plt.title('Excitatory Uniform Weights')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Density')\n",
        "plt.grid(True)\n",
        "\n",
        "# Inhibitory uniform weights\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(weights_inhibitory_np, bins=50, density=True, alpha=0.7, color='green', edgecolor='black')\n",
        "plt.title('Inhibitory Uniform Weights')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Density')\n",
        "plt.grid(True)\n",
        "\n",
        "# Unsigned uniform weights\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(weights_unsigned_np, bins=100, density=True, alpha=0.7, color='orange', edgecolor='black')\n",
        "plt.title('Unsigned Uniform Weights')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Density')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G25CJIf5h3sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "3JpvNoLVE_rU"
      },
      "source": [
        "***Can you think of more kinds of weight initializations and constraints that might be useful for the swimmer agent?***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "xPatIdlkE_rV"
      },
      "source": [
        "#### Section 3.1.2: Defining the ***SwimmerModule***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ovZmbltZE_rV"
      },
      "source": [
        "The ***SwimmerModule*** class represents the neural network module inspired by the C. elegans neural circuitry, designed for controlling a robotic swimmer with specific architectural priors, such as proprioception and oscillatory movement patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "uIMYn__lE_rV"
      },
      "outputs": [],
      "source": [
        "class SwimmerModule(nn.Module):\n",
        "    \"\"\"C.-elegans-inspired neural circuit architectural prior.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_joints: int,\n",
        "            log_dir: str,\n",
        "            log_file: str = None,\n",
        "            n_turn_joints: int = 1,\n",
        "            oscillator_period: int = 60,\n",
        "            use_weight_sharing: bool = True,\n",
        "            use_weight_constraints: bool = True,\n",
        "            use_weight_constant_init: bool = True,\n",
        "            include_proprioception: bool = True,\n",
        "            include_head_oscillators: bool = True,\n",
        "            include_speed_control: bool = False,\n",
        "            include_turn_control: bool = False,\n",
        "            include_contra_feedback_control: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.n_joints = n_joints\n",
        "        self.n_turn_joints = n_turn_joints\n",
        "        self.oscillator_period = oscillator_period\n",
        "        self.include_proprioception = include_proprioception\n",
        "        self.include_head_oscillators = include_head_oscillators\n",
        "        self.include_speed_control = include_speed_control\n",
        "        self.include_turn_control = include_turn_control\n",
        "        self.include_contra_feedback_control = include_turn_control\n",
        "        self.log_dir = log_dir\n",
        "        self.log_file = os.path.join(self.log_dir, 'activity_log.csv')\n",
        "\n",
        "        # Log activity\n",
        "        self.connections_log = []\n",
        "\n",
        "        # Timestep counter (for oscillations).\n",
        "        self.timestep = 0\n",
        "\n",
        "        # Weight sharing switch function.\n",
        "        self.ws = lambda nonshared, shared: shared if use_weight_sharing else nonshared\n",
        "\n",
        "        # Weight constraint and init functions.\n",
        "        if use_weight_constraints:\n",
        "            self.exc = excitatory\n",
        "            self.inh = inhibitory\n",
        "            if use_weight_constant_init:\n",
        "                exc_param = excitatory_constant\n",
        "                inh_param = inhibitory_constant\n",
        "            else:\n",
        "                exc_param = excitatory_uniform\n",
        "                inh_param = inhibitory_uniform\n",
        "        else:\n",
        "            self.exc = unsigned\n",
        "            self.inh = unsigned\n",
        "            if use_weight_constant_init:\n",
        "                exc_param = inh_param = unsigned_constant\n",
        "            else:\n",
        "                exc_param = inh_param = unsigned_uniform\n",
        "\n",
        "        # Learnable parameters.\n",
        "        self.params = nn.ParameterDict()\n",
        "        if use_weight_sharing:\n",
        "            if self.include_proprioception:\n",
        "                self.params['bneuron_prop'] = exc_param()\n",
        "            if self.include_speed_control:\n",
        "                self.params['bneuron_speed'] = inh_param()\n",
        "            if self.include_turn_control:\n",
        "                self.params['bneuron_turn'] = exc_param()\n",
        "            if self.include_head_oscillators:\n",
        "                self.params['bneuron_osc'] = exc_param()\n",
        "            self.params['muscle_ipsi'] = exc_param()\n",
        "            self.params['muscle_contra'] = inh_param()\n",
        "        else:\n",
        "            for i in range(self.n_joints):\n",
        "                if self.include_proprioception and i > 0:\n",
        "                    self.params[f'bneuron_d_prop_{i}'] = exc_param()\n",
        "                    self.params[f'bneuron_v_prop_{i}'] = exc_param()\n",
        "\n",
        "                if self.include_speed_control:\n",
        "                    self.params[f'bneuron_d_speed_{i}'] = inh_param()\n",
        "                    self.params[f'bneuron_v_speed_{i}'] = inh_param()\n",
        "\n",
        "                if self.include_turn_control and i < self.n_turn_joints:\n",
        "                    self.params[f'bneuron_d_turn_{i}'] = exc_param()\n",
        "                    self.params[f'bneuron_v_turn_{i}'] = exc_param()\n",
        "\n",
        "                if self.include_head_oscillators and i == 0:\n",
        "                    self.params[f'bneuron_d_osc_{i}'] = exc_param()\n",
        "                    self.params[f'bneuron_v_osc_{i}'] = exc_param()\n",
        "\n",
        "                self.params[f'muscle_d_d_{i}'] = exc_param()\n",
        "                self.params[f'muscle_d_v_{i}'] = inh_param()\n",
        "                self.params[f'muscle_v_v_{i}'] = exc_param()\n",
        "                self.params[f'muscle_v_d_{i}'] = inh_param()\n",
        "\n",
        "    def reset(self):\n",
        "        self.timestep = 0\n",
        "\n",
        "    def log_activity(self, activity_type, neuron):\n",
        "      \"\"\"Logs an active connection between neurons.\"\"\"\n",
        "      # adding file for connections in file\n",
        "      self.connections_log.append((self.timestep, activity_type, neuron))\n",
        "      # # Check if the directory exists\n",
        "      # if os.path.isdir(self.log_dir):\n",
        "      #   # Check if the log file exists\n",
        "      #   if os.path.exists(self.log_file):\n",
        "      #     # If the file exists, append the new log entry\n",
        "      #     with open(self.log_file, 'a', newline='') as f:\n",
        "      #       writer = csv.writer(f)\n",
        "      #       writer.writerow((self.timestep, activity_type, neuron))\n",
        "      #   else:\n",
        "      #     # If the file does not exist, create it and write the header and log entries\n",
        "      #     with open(self.log_file, 'w', newline='') as f:\n",
        "      #       writer = csv.writer(f)\n",
        "      #       writer.writerow([\"Timestep\", \"ActivityType\", \"Neuron\"])\n",
        "      #       # Write entries from connection_logs if not empty\n",
        "      #       if self.connections_log:\n",
        "      #         for log_entry in self.connections_log:\n",
        "      #           writer.writerow(log_entry)\n",
        "      # else:\n",
        "      #     # raise an error if the directory does not exist\n",
        "      #     pass\n",
        "\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            joint_pos,\n",
        "            right_control=None,\n",
        "            left_control=None,\n",
        "            speed_control=None,\n",
        "            timesteps=None,\n",
        "            log_activity=True,\n",
        "    ):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "    Args:\n",
        "      joint_pos (torch.Tensor): Joint positions in [-1, 1], shape (..., n_joints).\n",
        "      right_control (torch.Tensor): Right turn control in [0, 1], shape (..., 1).\n",
        "      left_control (torch.Tensor): Left turn control in [0, 1], shape (..., 1).\n",
        "      speed_control (torch.Tensor): Speed control in [0, 1], 0 stopped, 1 fastest, shape (..., 1).\n",
        "      timesteps (torch.Tensor): Timesteps in [0, max_env_steps], shape (..., 1).\n",
        "\n",
        "    Returns:\n",
        "      (torch.Tensor): Joint torques in [-1, 1], shape (..., n_joints).\n",
        "    \"\"\"\n",
        "\n",
        "        exc = self.exc\n",
        "        inh = self.inh\n",
        "        ws = self.ws\n",
        "\n",
        "        # Separate into dorsal and ventral sensor values in [0, 1], shape (..., n_joints).\n",
        "        joint_pos_d = joint_pos.clamp(min=0, max=1)\n",
        "        joint_pos_v = joint_pos.clamp(min=-1, max=0).neg()\n",
        "\n",
        "        # Convert speed signal from acceleration into brake.\n",
        "        if self.include_speed_control:\n",
        "            assert speed_control is not None\n",
        "            speed_control = 1 - speed_control.clamp(min=0, max=1)\n",
        "\n",
        "        joint_torques = []  # [shape (..., 1)]\n",
        "        for i in range(self.n_joints):\n",
        "            bneuron_d = bneuron_v = torch.zeros_like(joint_pos[..., 0, None])  # shape (..., 1)\n",
        "\n",
        "            # B-neurons recieve proprioceptive input from previous joint to propagate waves down the body.\n",
        "            if self.include_proprioception and i > 0:\n",
        "                bneuron_d = bneuron_d + joint_pos_d[\n",
        "                    ..., i - 1, None] * exc(self.params[ws(f'bneuron_d_prop_{i}', 'bneuron_prop')])\n",
        "                bneuron_v = bneuron_v + joint_pos_v[\n",
        "                    ..., i - 1, None] * exc(self.params[ws(f'bneuron_v_prop_{i}', 'bneuron_prop')])\n",
        "                self.log_activity('exc', f'bneuron_d_prop_{i}')\n",
        "                self.log_activity('exc', f'bneuron_v_prop_{i}')\n",
        "                # adding in the new variables for contra muscles for dorsal and ventral\n",
        "                if self.include_contra_feedback_control:\n",
        "                  contra_feedback = bneuron_d + bneuron_v\n",
        "\n",
        "\n",
        "            # Speed control unit modulates all B-neurons.\n",
        "            if self.include_speed_control:\n",
        "                bneuron_d = bneuron_d + speed_control * inh(\n",
        "                    self.params[ws(f'bneuron_d_speed_{i}', 'bneuron_speed')]\n",
        "                )\n",
        "                bneuron_v = bneuron_v + speed_control * inh(\n",
        "                    self.params[ws(f'bneuron_v_speed_{i}', 'bneuron_speed')]\n",
        "                )\n",
        "                self.log_activity('inh', f'bneuron_d_speed_{i}')\n",
        "                self.log_activity('inh', f'bneuron_v_speed_{i}')\n",
        "\n",
        "            # Turn control units modulate head B-neurons.\n",
        "            if self.include_turn_control and i < self.n_turn_joints:\n",
        "                assert right_control is not None\n",
        "                assert left_control is not None\n",
        "                turn_control_d = right_control.clamp(min=0, max=1)  # shape (..., 1)\n",
        "                turn_control_v = left_control.clamp(min=0, max=1)\n",
        "                bneuron_d = bneuron_d + turn_control_d * exc(\n",
        "                    self.params[ws(f'bneuron_d_turn_{i}', 'bneuron_turn')]\n",
        "                )\n",
        "                bneuron_v = bneuron_v + turn_control_v * exc(\n",
        "                    self.params[ws(f'bneuron_v_turn_{i}', 'bneuron_turn')]\n",
        "                )\n",
        "                self.log_activity('exc', f'bneuron_d_turn_{i}')\n",
        "                self.log_activity('exc', f'bneuron_v_turn_{i}')\n",
        "\n",
        "            # Oscillator units modulate first B-neurons.\n",
        "            if self.include_head_oscillators and i == 0:\n",
        "                if timesteps is not None:\n",
        "                    phase = timesteps.round().remainder(self.oscillator_period)\n",
        "                    mask = phase < self.oscillator_period // 2\n",
        "                    oscillator_d = torch.zeros_like(timesteps)  # shape (..., 1)\n",
        "                    oscillator_v = torch.zeros_like(timesteps)  # shape (..., 1)\n",
        "                    oscillator_d[mask] = 1.\n",
        "                    oscillator_v[~mask] = 1.\n",
        "                else:\n",
        "                    phase = self.timestep % self.oscillator_period  # in [0, oscillator_period)\n",
        "                    if phase < self.oscillator_period // 2:\n",
        "                        oscillator_d, oscillator_v = 1.0, 0.0\n",
        "                    else:\n",
        "                        oscillator_d, oscillator_v = 0.0, 1.0\n",
        "                bneuron_d = bneuron_d + oscillator_d * exc(\n",
        "                    self.params[ws(f'bneuron_d_osc_{i}', 'bneuron_osc')]\n",
        "                )\n",
        "                bneuron_v = bneuron_v + oscillator_v * exc(\n",
        "                    self.params[ws(f'bneuron_v_osc_{i}', 'bneuron_osc')]\n",
        "                )\n",
        "\n",
        "                self.log_activity('exc', f'bneuron_d_osc_{i}')\n",
        "                self.log_activity('exc', f'bneuron_v_osc_{i}')\n",
        "\n",
        "            # B-neuron activation.\n",
        "            bneuron_d = graded(bneuron_d)\n",
        "            bneuron_v = graded(bneuron_v)\n",
        "\n",
        "            # Muscles receive excitatory ipsilateral and inhibitory contralateral input.\n",
        "            muscle_d = graded(\n",
        "                bneuron_d * exc(self.params[ws(f'muscle_d_d_{i}', 'muscle_ipsi')]) +\n",
        "                bneuron_v * inh(self.params[ws(f'muscle_d_v_{i}', 'muscle_contra')])\n",
        "            )\n",
        "            if self.include_contra_feedback_control:\n",
        "              muscle_d = muscle_d + contra_feedback\n",
        "            muscle_v = graded(\n",
        "                bneuron_v * exc(self.params[ws(f'muscle_v_v_{i}', 'muscle_ipsi')]) +\n",
        "                bneuron_d * inh(self.params[ws(f'muscle_v_d_{i}', 'muscle_contra')])\n",
        "            )\n",
        "            if self.include_contra_feedback_control:\n",
        "              muscle_v = muscle_v + contra_feedback\n",
        "\n",
        "            # Joint torque from antagonistic contraction of dorsal and ventral muscles.\n",
        "            joint_torque = muscle_d - muscle_v\n",
        "            joint_torques.append(joint_torque)\n",
        "\n",
        "        self.timestep += 1\n",
        "\n",
        "        out = torch.cat(joint_torques, -1)  # shape (..., n_joints)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ex-m5koJE_rV"
      },
      "source": [
        "#### Section 3.1.3: Defining the ***SwimmerActor*** wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "nxDXaKdbE_rV"
      },
      "source": [
        "\n",
        "The ***SwimmerActor*** class acts as a wrapper around the ***SwimmerModule***, managing high-level control signals and observations coming from the environment and passing them to the ***SwimmerModule*** in a suitable format. This class is basically responsible for making the SwimmerModule compatible with the tonic library. If you wish to use any other library to try a algorithm not present in tonic you have to write a new wrapper to make ***SwimmerModule*** compatible with that library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "cie8Lo9NE_rV"
      },
      "outputs": [],
      "source": [
        "class SwimmerActor(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            swimmer,\n",
        "            controller=None,\n",
        "            distribution=None,\n",
        "            timestep_transform=(-1, 1, 0, 1000),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.swimmer = swimmer\n",
        "        self.controller = controller\n",
        "        self.distribution = distribution\n",
        "        self.timestep_transform = timestep_transform\n",
        "\n",
        "    def initialize(\n",
        "            self,\n",
        "            observation_space,\n",
        "            action_space,\n",
        "            observation_normalizer=None,\n",
        "    ):\n",
        "        self.action_size = action_space.shape[0]\n",
        "\n",
        "    def forward(self, observations):\n",
        "        joint_pos = observations[..., :self.action_size]\n",
        "        timesteps = observations[..., -1, None]\n",
        "\n",
        "        # Normalize joint positions by max joint angle (in radians).\n",
        "        joint_limit = 2 * np.pi / (self.action_size + 1)  # In dm_control, calculated with n_bodies.\n",
        "        joint_pos = torch.clamp(joint_pos / joint_limit, min=-1, max=1)\n",
        "\n",
        "        # Convert normalized time signal into timestep.\n",
        "        if self.timestep_transform:\n",
        "            low_in, high_in, low_out, high_out = self.timestep_transform\n",
        "            timesteps = (timesteps - low_in) / (high_in - low_in) * (high_out - low_out) + low_out\n",
        "\n",
        "        # Generate high-level control signals.\n",
        "        if self.controller:\n",
        "            right, left, speed = self.controller(observations)\n",
        "        else:\n",
        "            right, left, speed = None, None, None\n",
        "\n",
        "        # Generate low-level action signals.\n",
        "        actions = self.swimmer(\n",
        "            joint_pos,\n",
        "            timesteps=timesteps,\n",
        "            right_control=right,\n",
        "            left_control=left,\n",
        "            speed_control=speed,\n",
        "        )\n",
        "\n",
        "        # Pass through distribution for stochastic policy.\n",
        "        if self.distribution:\n",
        "            actions = self.distribution(actions)\n",
        "\n",
        "        return actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "ZFTj8FFHE_rV"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_ncap_classes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "32SeFcWhE_rV"
      },
      "source": [
        "### 3.2: Train NCAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "rrQA0jMpE_rV"
      },
      "source": [
        "We will now define functions akin to those for MLP we defined in Section 3.2, but tailored for the SwimmerActor model.\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "-v3-1BBTE_rV"
      },
      "outputs": [],
      "source": [
        "from tonic.torch import models, normalizers\n",
        "import torch\n",
        "\n",
        "\n",
        "def ppo_swimmer_model(\n",
        "        n_joints=5,\n",
        "        action_noise=0.1,\n",
        "        critic_sizes=(64, 64),\n",
        "        critic_activation=nn.Tanh,\n",
        "        **swimmer_kwargs,\n",
        "):\n",
        "    return models.ActorCritic(\n",
        "        actor=SwimmerActor(\n",
        "            swimmer=SwimmerModule(n_joints=n_joints, **swimmer_kwargs),\n",
        "            distribution=lambda x: torch.distributions.normal.Normal(x, action_noise),\n",
        "        ),\n",
        "        critic=models.Critic(\n",
        "            encoder=models.ObservationEncoder(),\n",
        "            torso=models.MLP(critic_sizes, critic_activation),\n",
        "            head=models.ValueHead(),\n",
        "        ),\n",
        "        observation_normalizer=normalizers.MeanStd(),\n",
        "    )\n",
        "\n",
        "\n",
        "def d4pg_swimmer_model(\n",
        "  activity_dir_file_path: str,\n",
        "  n_joints=5,\n",
        "  critic_sizes=(256, 256),\n",
        "  critic_activation=nn.ReLU,\n",
        "  **swimmer_kwargs,\n",
        "):\n",
        "  # NOTE: swimmer kwargs empty -- what modifications/params can be used.\n",
        "  return models.ActorCriticWithTargets(\n",
        "    actor=SwimmerActor(swimmer=SwimmerModule(n_joints=n_joints, log_dir=activity_dir_file_path, **swimmer_kwargs),),\n",
        "    critic=models.Critic(\n",
        "      encoder=models.ObservationActionEncoder(),\n",
        "      torso=models.MLP(critic_sizes, critic_activation),\n",
        "      # These values are for the control suite with 0.99 discount.\n",
        "      head=models.DistributionalValueHead(-150., 150., 51),\n",
        "    ),\n",
        "    observation_normalizer=normalizers.MeanStd(),\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "steps = '2e5'\n",
        "experiment_name = f'ncap_ddpg_baseline_steps_{steps}'\n",
        "activity_dir_file_path = os.path.join('/content/drive/My Drive/', 'data', 'experiments', 'tonic', \"swimmer-swim\", experiment_name)"
      ],
      "metadata": {
        "id": "y18CihI84sXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "XNlb0pVUE_rV"
      },
      "source": [
        "⏳"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "-xZLXS_eE_rV"
      },
      "outputs": [],
      "source": [
        "train('import tonic.torch',\n",
        "      'tonic.torch.agents.D4PG(model=d4pg_swimmer_model(n_joints=5, activity_dir_file_path=activity_dir_file_path, critic_sizes=(128,128)))',\n",
        "      #'tonic.torch.agents.PPO(model=ppo_swimmer_model(n_joints=5,critic_sizes=(256,256)))',\n",
        "  'tonic.environments.ControlSuite(\"swimmer-swim\",time_feature=True)',\n",
        "  name = experiment_name,\n",
        "  trainer = 'tonic.Trainer(steps=int(2e5),save_steps=int(5e4))')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "xqKltaR5E_rV"
      },
      "source": [
        "Let's visualize the trained NCAP agent in the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "dwaYxdFcE_rV"
      },
      "source": [
        "⏳"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "4HFEFVICE_rW"
      },
      "outputs": [],
      "source": [
        "play_model('/content/drive/My Drive/data/experiments/tonic/swimmer-swim/ncap_ddpg_baseline_steps_2e5')\n",
        "#play_model('/content/drive/My Drive/data/experiments/tonic/swimmer-swim/ncap_ddpg_baseline_steps_6e5/')\n",
        "#play_model('data/local/experiments/tonic/swimmer-swim/pretrained_ncap_ppo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "qSz17k3HE_rW"
      },
      "source": [
        "❓"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "TpOleKHzE_rW"
      },
      "source": [
        "***This architecture was designed using the C. elegans motor circuit that can swim right at birth i.e it should already have really good priors. Can you try visualizing an agent with an untrained NCAP model. Can it swim?***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "YkLJpVNuE_rW"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_train_ncap\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "96yAHGIfE_rW"
      },
      "source": [
        "### 3.3 Plot perfomance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "5tXLbKi-E_rW"
      },
      "source": [
        "Now we are going to visualize performance of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "ilDGsiPcE_rW"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objs as go\n",
        "\n",
        "\n",
        "def plot_performance(paths, title='Model Performance'):\n",
        "    \"\"\"\n",
        "    Plots the performance of multiple models on the same axes using Plotly for interactive visualization.\n",
        "\n",
        "    Reads CSV log files from specified paths and plots the mean episode scores\n",
        "    achieved during testing against the cumulative time steps for each model.\n",
        "    The plot uses a logarithmic scale for the x-axis to better display the progression\n",
        "    over a wide range of steps. Each line's legend is set to the name of the last folder\n",
        "    in the path, representing the model's name.\n",
        "\n",
        "    Parameters:\n",
        "    - paths (list of str): Paths to the experiment directories.\n",
        "    \"\"\"\n",
        "    fig = go.Figure()\n",
        "\n",
        "    for index, path in enumerate(paths):\n",
        "        # Extract the model name from the path\n",
        "        model_name = os.path.basename(path.rstrip('/'))\n",
        "\n",
        "        # Load data\n",
        "        df = pd.read_csv(os.path.join(path, 'log.csv'))\n",
        "        scores = df['test/episode_score/mean']\n",
        "        lengths = df['test/episode_length/mean']\n",
        "        scores_min = df['test/episode_score/min']\n",
        "        scores_max = df['test/episode_score/max']\n",
        "        steps = np.cumsum(lengths)\n",
        "\n",
        "        # Add line plot for mean scores\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=steps, y=scores,\n",
        "            mode='lines',\n",
        "            name=f\"{model_name} mean\",\n",
        "            line=dict(color=f'rgba({index*50 % 255},{index*100 % 255},{index*150 % 255},1)')\n",
        "        ))\n",
        "\n",
        "        # Add shaded area for min and max scores\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=np.concatenate([steps, steps[::-1]]),\n",
        "            y=np.concatenate([scores_max, scores_min[::-1]]),\n",
        "            fill='toself',\n",
        "            fillcolor=f'rgba({index*50 % 255},{index*100 % 255},{index*150 % 255},0.2)',\n",
        "            line=dict(color='rgba(255,255,255,0)'),\n",
        "            hoverinfo=\"skip\",\n",
        "            showlegend=False\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis=dict(title='Cumulative Time Steps', type='log', tickvals=[1e4, 1e5, 1e6], ticktext=['10^4', '10^5', '10^6']),\n",
        "        yaxis=dict(title='Episode Avg Score'),\n",
        "        legend_title='Models',\n",
        "        template='plotly_white'\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "def plot_actor_critic_loss(paths, title='Actor/Critic Loss'):\n",
        "    fig = go.Figure()\n",
        "    for index, path in enumerate(paths):\n",
        "        # Extract the model name from the path\n",
        "        model_name = os.path.basename(path.rstrip('/'))\n",
        "        # Load data\n",
        "        df = pd.read_csv(os.path.join(path, 'log.csv'))\n",
        "        actor_loss = df['actor/loss']\n",
        "        critic_loss = df['critic/loss']\n",
        "        lengths = df['test/episode_length/mean']\n",
        "        steps = np.cumsum(lengths)\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=steps, y=actor_loss,\n",
        "            mode='lines',\n",
        "            name=f\"{model_name} actor loss\",\n",
        "            line=dict(color=f'rgba({index*50 % 255},{index*100 % 255},{index*150 % 255},1)')\n",
        "        ))\n",
        "\n",
        "        # Add line plot for critic loss\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=steps, y=critic_loss,\n",
        "            mode='lines',\n",
        "            name=f\"{model_name} critic loss\",\n",
        "            line=dict(dash='dash', color=f'rgba({(index+1)*50 % 255},{(index+1)*100 % 255},{(index+1)*150 % 255},0.5)')\n",
        "        ))\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis=dict(title='Cumulative Time Steps', type='log', tickvals=[1e4, 1e5, 1e6], ticktext=['10^4', '10^5', '10^6']),\n",
        "        yaxis=dict(title='Actor/Critic Loss'),\n",
        "        legend_title='Models',\n",
        "        template='plotly_white'\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def plot_performance_matplotlib(paths, ax=None,title='Model Performance'):\n",
        "    \"\"\"\n",
        "    Plots the performance of multiple models on the same axes using Seaborn for styling.\n",
        "\n",
        "    Reads CSV log files from specified paths and plots the mean episode scores\n",
        "    achieved during testing against the cumulative time steps for each model.\n",
        "    The plot uses a logarithmic scale for the x-axis to better display the progression\n",
        "    over a wide range of steps. Each line's legend is set to the name of the last folder\n",
        "    in the path, representing the model's name. Seaborn styles are applied for enhanced visualization.\n",
        "\n",
        "    Parameters:\n",
        "    - paths (list of str): Paths to the experiment directories.\n",
        "    - ax (matplotlib.axes.Axes, optional): A matplotlib axis object to plot on. If None,\n",
        "      a new figure and axis are created.\n",
        "    \"\"\"\n",
        "    # Set the Seaborn style\n",
        "    sns.set(style=\"whitegrid\")\n",
        "    colors = sns.color_palette(\"colorblind\")  # Colorblind-friendly palette\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "    for index, path in enumerate(paths):\n",
        "        # Extract the model name from the path\n",
        "        model_name = os.path.basename(path.rstrip('/'))\n",
        "\n",
        "        # Load data\n",
        "        df = pd.read_csv(os.path.join(path, 'log.csv'))\n",
        "        scores = df['test/episode_score/mean']\n",
        "        lengths = df['test/episode_length/mean']\n",
        "        scores_min = df['test/episode_score/min']\n",
        "        scores_max = df['test/episode_score/max']\n",
        "        steps = np.cumsum(lengths)\n",
        "        sns.lineplot(x=steps, y=scores, ax=ax, label=model_name, color=colors[index % len(colors)])\n",
        "        ax.fill_between(steps, scores_min, scores_max, color=colors[index % len(colors)], alpha=0.3)\n",
        "\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_xlabel('Cumulative Time Steps')\n",
        "    ax.set_ylabel('average episode score')\n",
        "    ax.legend()\n",
        "    ax.set_title(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "QuyeNFmwE_rW"
      },
      "outputs": [],
      "source": [
        "#fig, ax = plt.subplots()\n",
        "\n",
        "#Replace the paths with the path to models you trained to plot their performance.\n",
        "# paths = [\n",
        "#     'data/local/experiments/tonic/swimmer-swim/pretrained_ncap_ppo',\n",
        "#     'data/local/experiments/tonic/swimmer-swim/pretrained_mlp_ppo'\n",
        "# ]\n",
        "# ncap plots ncap_ddpg_exp_critic_size256_6e5\n",
        "paths = [\n",
        "    '/content/drive/My Drive/data/experiments/tonic/swimmer-swim/ncap_ddpg_baseline_steps_2e5/',\n",
        "]\n",
        "plot_performance(paths, title='NCAP models comparison with uniform distribution')\n",
        "#plot_performance_matplotlib(paths, ax=ax, title='NCAP models comparison with uniform distribution')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_paths = [\n",
        "    '/content/drive/My Drive/data/experiments/tonic/swimmer-swim/ncap_ddpg_baseline_steps_2e5',]\n",
        "plot_actor_critic_loss(loss_paths)"
      ],
      "metadata": {
        "id": "RYsg8kWUQqa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_paths = [\n",
        "    '/content/drive/My Drive/data/experiments/tonic/swimmer-swim/ncap_ddpg_exp_critic_size256_6e5',]\n",
        "plot_actor_critic_loss(loss_paths)"
      ],
      "metadata": {
        "id": "EfUwTXXQSi5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "EH6jQFwAE_rW"
      },
      "source": [
        "❓"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "SOjwonnqE_rW"
      },
      "source": [
        "\n",
        "\n",
        "* ***Compare the performance and learning curve of NCAP to MLP for the basic swimmer agent.***\n",
        "* ***Try testing the model on a modification of the environment (e.g., the 12-link swimmer) it was trained on.***\n",
        "* ***What happens if we remove certain weight constraints (e.g., sign constraint) from the NCAP model?***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "qUw-JnhsE_rW"
      },
      "outputs": [],
      "source": [
        "# add your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "bb75-iveE_rW"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_plot_performance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "VSlhWnFWE_rW"
      },
      "source": [
        "---\n",
        "## Section 4: Visualizing the sparse network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "gcFu14yGE_rX"
      },
      "source": [
        "\n",
        "Given the importance of architectural choices in our project we have provided a function which can visualize the network architecture. This includes the ability to render the NCAP network, representing the C. Elegans connectome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "UDNupdi2E_rX"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def draw_network(mode='NCAP', N=2, include_speed_control=False, include_turn_control=False):\n",
        "\n",
        "    \"\"\"\n",
        "    Draws a network graph for a swimmer model based on either NCAP or MLP architecture.\n",
        "\n",
        "    Parameters:\n",
        "    - mode (str): Determines the architecture type ('NCAP' or 'MLP'). Defaults to 'NCAP'.\n",
        "    - N (int): Number of joints in the swimmer model. Defaults to 2.\n",
        "    - include_speed_control (bool): If True, includes nodes for speed control in the graph.\n",
        "    - include_turn_control (bool): If True, includes nodes for turn control in the graph.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    n=2+N*4\n",
        "\n",
        "    nodes =dict()\n",
        "\n",
        "    if include_speed_control:\n",
        "      nodes['1-s'] = n+7\n",
        "    if include_turn_control:\n",
        "      nodes['r'] = n+5\n",
        "      nodes['l'] = n+3\n",
        "\n",
        "    nodes['o'] = n-1\n",
        "    nodes['$o^d$'] = n-0\n",
        "    nodes['$o^v$']= n-2\n",
        "\n",
        "    custom_node_positions = {}\n",
        "    custom_node_positions['o'] = (1, nodes['o'])\n",
        "    custom_node_positions['$o^d$'] = (1.5, nodes['$o^d$'])\n",
        "    custom_node_positions['$o^v$'] = (1.5, nodes['$o^v$'])\n",
        "\n",
        "\n",
        "    if include_speed_control:\n",
        "      custom_node_positions['1-s'] = (1.5, nodes['1-s'])\n",
        "    if include_turn_control:\n",
        "      custom_node_positions['r'] = (1.5, nodes['r'])\n",
        "      custom_node_positions['l'] = (1.5, nodes['l'])\n",
        "\n",
        "    for i in range(1,N+1):\n",
        "      nodes[f'$q_{i}$'] = 4*(N-i) + 1\n",
        "      nodes[f'$q^d_{i}$'] = 4*(N-i) + 2\n",
        "      nodes[f'$q^v_{i}$'] = 4*(N-i)\n",
        "      nodes[f'$b^d_{i}$'] = 4*(N-i) + 2\n",
        "      nodes[f'$b^v_{i}$'] = 4*(N-i)\n",
        "      nodes[f'$m^d_{i}$'] = 4*(N-i) + 2\n",
        "      nodes[f'$m^v_{i}$'] = 4*(N-i)\n",
        "      nodes['$\\overset{..}{q}$' + f'$_{i}$'] = 4*(N-i) + 1\n",
        "\n",
        "      custom_node_positions[f'$q_{i}$'] = (1, nodes[f'$q_{i}$'])\n",
        "      custom_node_positions[f'$q^d_{i}$'] = (1.5, nodes[f'$q^d_{i}$'])\n",
        "      custom_node_positions[f'$q^v_{i}$'] = (1.5, nodes[f'$q^v_{i}$'])\n",
        "      custom_node_positions[f'$b^d_{i}$'] = (2, nodes[f'$b^d_{i}$'])\n",
        "      custom_node_positions[f'$b^v_{i}$'] = (2, nodes[f'$b^v_{i}$'])\n",
        "      custom_node_positions[f'$m^d_{i}$'] = (2.5, nodes[f'$m^d_{i}$'])\n",
        "      custom_node_positions[f'$m^v_{i}$'] = (2.5, nodes[f'$m^v_{i}$'])\n",
        "      custom_node_positions['$\\overset{..}{q}$' + f'$_{i}$'] = (3, nodes['$\\overset{..}{q}$' + f'$_{i}$'])\n",
        "\n",
        "    for node, layer in nodes.items():\n",
        "        G.add_node(node, layer=layer)\n",
        "\n",
        "    if mode=='NCAP':\n",
        "        # Add edges between nodes\n",
        "        edges_colors = ['green', 'orange', 'green', 'green']\n",
        "        edge_labels = {\n",
        "            ('o', '$o^d$'):'+1',\n",
        "            ('o', '$o^v$'):'-1',\n",
        "            ('$o^d$', '$b^d_1$'):'o',\n",
        "            ('$o^v$', '$b^v_1$'):'o'\n",
        "            }\n",
        "\n",
        "        if include_speed_control:\n",
        "          edges_colors += ['orange']\n",
        "          edge_labels[('1-s', '$b^d_1$')] = 's, to all b'\n",
        "        if include_turn_control:\n",
        "          edges_colors += ['green', 'green']\n",
        "          edge_labels[('r', '$b^d_1$')] = 't'\n",
        "          edge_labels[('l', '$b^v_1$')] = 't'\n",
        "\n",
        "\n",
        "        for i in range(1,N+1):\n",
        "          if i < N:\n",
        "            edges_colors += ['green', 'orange', 'green', 'green']\n",
        "\n",
        "            edge_labels[((f'$q_{i}$', f'$q^d_{i}$'))] = '+1'\n",
        "            edge_labels[((f'$q_{i}$', f'$q^v_{i}$'))] = '-1'\n",
        "            edge_labels[((f'$q^d_{i}$', f'$b^d_{i+1}$'))] = 'p'\n",
        "            edge_labels[((f'$q^v_{i}$', f'$b^v_{i+1}$'))] = 'p'\n",
        "\n",
        "          edges_colors += ['green', 'orange', 'green', 'orange',\n",
        "                          'orange', 'green']\n",
        "\n",
        "          edge_labels[((f'$b^d_{i}$', f'$m^d_{i}$'))] = 'i'\n",
        "          edge_labels[((f'$b^d_{i}$', f'$m^v_{i}$'))] = 'c'\n",
        "          edge_labels[((f'$b^v_{i}$', f'$m^v_{i}$'))] = 'i'\n",
        "          edge_labels[((f'$b^v_{i}$', f'$m^d_{i}$'))] = 'c'\n",
        "          edge_labels[((f'$m^v_{i}$', '$\\overset{..}{q}$' + f'$_{i}$'))] = '-1'\n",
        "          edge_labels[((f'$m^d_{i}$', '$\\overset{..}{q}$' + f'$_{i}$'))] = '+1'\n",
        "\n",
        "        edges = edge_labels.keys()\n",
        "\n",
        "    elif mode=='MLP':\n",
        "      edges = []\n",
        "      layers = [1, 1.5, 2, 2.5, 3]\n",
        "      layers_nodes = [[], [], [], [], []]\n",
        "      for key, value in custom_node_positions.items():\n",
        "        ind = layers.index(value[0])\n",
        "        layers_nodes[ind].append(key)\n",
        "      for layer_ind in range(len(layers_nodes) - 1):\n",
        "        for node1 in layers_nodes[layer_ind]:\n",
        "          for node2 in layers_nodes[layer_ind+1]:\n",
        "            edges.append((node1, node2))\n",
        "      edges_colors = np.repeat('gray', len(edges))\n",
        "\n",
        "\n",
        "    G.add_edges_from(edges)\n",
        "\n",
        "    # Draw the graph using the custom node positions\n",
        "    options = {\"edge_color\": edges_colors, \"edgecolors\": \"tab:gray\", \"node_size\": 500, 'node_color':'white'}\n",
        "    nx.draw(G, pos=custom_node_positions, with_labels=True, arrowstyle=\"-\", arrowsize=20, **options)\n",
        "    if mode=='NCAP':\n",
        "      nx.draw_networkx_edge_labels(G, pos=custom_node_positions, edge_labels=edge_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "_iNbXLhHE_rX"
      },
      "outputs": [],
      "source": [
        "draw_network('MLP', N=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "YZCxwEYKE_rX"
      },
      "outputs": [],
      "source": [
        "draw_network('NCAP', N=6, include_speed_control=True, include_turn_control=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "XME4yNBBE_rX"
      },
      "source": [
        "Notice that the NCAP architecture is highly sparse and interpretable as compared to the MLP. Moreover notice that the ncap architecture can be completely embedded within a fully connected MLP of 3 hidden layers and ReLU nonlinearities. This enables us to do a thorough investigation into how specific architectural elements influence both performance and the learning process. By leveraging this capability, we can systematically analyze the impact of the architectural preferences inherent to the model and make better design choices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "BCAcy1RqE_rX"
      },
      "source": [
        "*It might be useful to also visualize the network's activity for NCAP. Given it only has 4 learnable parameters it becomes much easier to interpret the network.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "o76Flo6LE_rX"
      },
      "outputs": [],
      "source": [
        "# @title Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_visualizing_the_sparse_network\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "CQ57hdVWE_rX"
      },
      "source": [
        "---\n",
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "u6XwTwLeE_rX"
      },
      "source": [
        "\n",
        "\n",
        "Based on the concepts we've discussed in this tutorial, you should now be equipped to advance with the project. Within this project, there are multiple pathways you can explore. For each pathway, you will delve deeply into one of the main sections outlined in this notebook, allowing for a thorough investigation of different factors that can influence performance:\n",
        "\n",
        "*   ***Exploring the effects of environment:*** Investigate how different environmental settings impact agent performance. This could involve altering parameters of the environment or the types of tasks and reward functions. Understanding these effects can help in making better architectural choices and learning algorithms that result in agents that are robust and adaptable\n",
        "\n",
        "\n",
        "*   ***Exploring the effects of learning algorithms:*** Standard RL algorithms often struggle with sparse and constrained networks, which can lead to suboptimal performance. Explore where these algorithms fail and analyze potential reasons for their limitations. Experiment with modifications or alternative algorithms that might overcome these challenges.\n",
        "\n",
        "\n",
        "* ***Exploring the effects of model architecture:*** Investigate how various architectural decisions within the NCAP model influence its performance. Visualize the model and its activity and explore potential improvements by tweaking architectural elements, assessing how these changes affect learning outcomes and operational efficiency."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "wCaEZAhrE_rR",
        "OZJnX2uRE_rS"
      ]
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}